# Zenith Framework Performance Optimizations Reference

*Comprehensive knowledge base for performance optimizations in the Zenith web framework*

## Overview

This document serves as the **permanent reference guide** for performance optimizations in Zenith. It provides:

- **Complete optimization techniques catalog** - All known Python/web framework optimizations
- **Implementation tracking** - What's been applied to Zenith and when  
- **Future opportunities** - Roadmap for continued performance improvements
- **Benchmarking standards** - Performance targets and measurement practices

**Use this document to:**
- Reference optimization techniques during development
- Plan performance improvement sprints
- Maintain Zenith's competitive performance edge
- Onboard developers on performance best practices

## ğŸ“š Complete Python Performance Optimization Techniques

*This section serves as a comprehensive reference for ALL known performance optimization techniques.*

### ğŸ”§ Core Python Optimizations

#### __slots__ for Memory Efficiency (40% memory reduction)
```python
class MyClass:
    __slots__ = ('attr1', 'attr2', 'attr3')  # Fixed attribute storage
    
# For dataclasses:
@dataclass(slots=True)
class MyDataClass:
    attr1: str
    attr2: int
```
**Benefits:** 40% memory reduction, faster attribute access, prevents dynamic attributes
**Use cases:** High-frequency objects, data containers, dependency injection classes

#### String Optimization Techniques (15-25% string performance)
```python
# String interning for frequent comparisons (use for HTTP constants)
HTTP_GET = sys.intern("GET")
HTTP_POST = sys.intern("POST")  # 15% faster comparisons

# F-strings - fastest formatting (stick with these)
message = f"User {user_id} has {count} items"  # Best performance + readability
error_msg = f"Request failed with status {status_code}"

# join() for multiple concatenations (vs + operator)
result = "".join([str1, str2, str3, str4])  # O(n) vs O(nÂ²) for +

# Avoid slower alternatives in hot paths:
# message = "User {} has {} items".format(user_id, count)  # Slower
# message = "User %s has %d items" % (user_id, count)      # Slower
```
**For Zenith:** Use f-strings everywhere. They're fast, readable, and stable.

#### Data Structure Optimization (O(1) vs O(n) improvements)
```python
# Frozensets for membership testing
ALLOWED_METHODS = frozenset(['GET', 'POST', 'PUT'])  # O(1) lookup
if method in ALLOWED_METHODS:  # Fast

# Dictionary comprehensions vs loops (15-30% faster)
filtered = {k: v for k, v in data.items() if condition(v)}  # Faster
# vs
filtered = {}
for k, v in data.items():
    if condition(v):
        filtered[k] = v  # Slower

# Use appropriate collections
from collections import defaultdict, deque, Counter
cache = defaultdict(list)  # Eliminates key checks
queue = deque()  # O(1) append/popleft vs list O(n)
counts = Counter(items)  # Optimized counting
```

#### Built-in Function Optimization (2-10x improvements)
```python
# Use built-ins instead of manual loops
total = sum(numbers)  # Fast C implementation
mapped = list(map(str.upper, strings))  # Fast if using built-in function
filtered = list(filter(lambda x: x > 0, numbers))  # Fast filtering

# List comprehensions vs explicit loops (2x faster)
squared = [x**2 for x in numbers]  # Faster
# vs
squared = []
for x in numbers:
    squared.append(x**2)  # Slower
```

#### Memory Management Techniques (15-25% GC reduction)
```python
# Generator expressions for large datasets
large_data = (process(item) for item in huge_list)  # Lazy evaluation
# vs
large_data = [process(item) for item in huge_list]  # Memory heavy

# Object pooling for frequent allocations
from collections import deque

class ObjectPool:
    def __init__(self, factory, max_size=100):
        self._pool = deque(maxlen=max_size)
        self._factory = factory
```

#### Caching and Memoization (25-90% speedup for repeated calls)
```python
from functools import lru_cache

@lru_cache(maxsize=256)
def expensive_computation(n):
    return complex_calculation(n)

# Custom caching with TTL
@cached(ttl=300)  # Zenith's custom decorator
async def database_query(user_id):
    return await db.get_user(user_id)
```

#### Regex Optimization (10-50x pattern matching)
```python
# Precompile regex at module level
PATH_PARAM = re.compile(r'\{([^}]+)\}')
EMAIL_PATTERN = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$')

# Use in hot paths
def extract_params(path):
    return PATH_PARAM.findall(path)  # Fast precompiled
```

### ğŸŒ Web Framework Optimizations

#### Pure ASGI vs BaseHTTPMiddleware (50-127% performance gain)
```python
# Efficient: Pure ASGI
class FastMiddleware:
    async def __call__(self, scope, receive, send):
        # Direct ASGI processing
        await self.app(scope, receive, send)

# Inefficient: BaseHTTPMiddleware
class SlowMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        return await call_next(request)  # Request/Response overhead
```

#### Connection Pool Optimization (15-25% DB overhead reduction)
```python
# Optimized database configuration
Database(
    pool_size=20,           # Match expected concurrent requests
    max_overflow=30,        # Handle spikes
    pool_timeout=30,        # Prevent hanging
    pool_recycle=3600,      # Refresh connections hourly
    pool_pre_ping=True      # Validate connections
)
```

#### Response Caching Strategies (25-40% response time reduction)
```python
# Method-level caching
@lru_cache(maxsize=1000)
def generate_response(content_hash: str) -> str:
    return expensive_template_render()

# Middleware-level caching
cache_middleware = CacheMiddleware(
    backend='redis',
    default_ttl=300,
    key_prefix='api:v1'
)
```

### âš¡ Async/IO Optimizations

#### TaskGroup vs asyncio.gather() (20-30% async improvement)
```python
# Efficient: TaskGroup (Python 3.11+)
async with asyncio.TaskGroup() as tg:
    task1 = tg.create_task(operation1())
    task2 = tg.create_task(operation2())
    # Better error handling and resource management

# Less efficient: gather()
results = await asyncio.gather(
    operation1(),
    operation2(),
    return_exceptions=True
)
```

#### Efficient JSON Serialization (2-10x JSON performance)
```python
# Fastest: msgspec (Zenith's choice)
import msgspec
encoder = msgspec.json.Encoder()
data = encoder.encode(payload)

# Alternative: orjson
import orjson
data = orjson.dumps(payload)

# Avoid: standard json in hot paths
import json
data = json.dumps(payload)  # Slowest
```

### ğŸŒŠ Pure ASGI Optimization Opportunities (v0.1.4+)

*With BaseHTTPMiddleware eliminated, these ASGI-native optimizations become available:*

#### Zero-Copy Streaming Operations (40-60% memory reduction for large payloads)
```python
async def streaming_middleware(scope, receive, send):
    """Handle large uploads without buffering in memory."""
    if scope.get("method") == "POST" and is_large_upload(scope):
        # Stream directly to storage - no intermediate buffering
        async for chunk in receive_body_stream(receive):
            await storage.write_chunk_async(chunk)
        
        await send({
            "type": "http.response.start",
            "status": 201,
            "headers": [[b"content-type", b"application/json"]],
        })
```

#### Concurrent Middleware Processing (20-30% middleware stack improvement)
```python
async def concurrent_auth_middleware(scope, receive, send):
    """Run authentication and rate limiting concurrently."""
    async with asyncio.TaskGroup() as tg:
        auth_task = tg.create_task(authenticate_user(scope))
        rate_task = tg.create_task(check_rate_limit(scope))
        
    # Both completed concurrently
    user = auth_task.result()
    rate_ok = rate_task.result()
```

#### Database Connection Reuse (15-25% database performance)
```python
async def db_connection_middleware(scope, receive, send):
    """Reuse AsyncPG connection throughout request lifecycle."""
    async with db_pool.acquire() as conn:
        scope["db_connection"] = conn
        await app(scope, receive, send)
        # Connection properly released after response sent
        # No thread pool overhead from BaseHTTPMiddleware
```

#### WebSocket Performance Improvements (15-25% WebSocket throughput)
```python
async def optimized_websocket_middleware(scope, receive, send):
    """Native ASGI WebSocket handling."""
    if scope["type"] == "websocket":
        # Direct protocol handling - no HTTP wrapper overhead
        websocket = WebSocket(scope, receive, send)
        await websocket_handler(websocket)
```

#### Server-Sent Events with Backpressure (Handle 10x larger concurrent streams)
```python
async def sse_middleware(scope, receive, send):
    """Efficient streaming responses with flow control."""
    if accepts_sse(scope):
        await send({"type": "http.response.start", "status": 200})
        
        async for event in event_stream():
            # Backpressure-aware streaming
            if not await check_client_buffer(send):
                await asyncio.sleep(0.1)  # Client can't keep up
            await send_sse_event(send, event)
```

#### HTTP/2 & HTTP/3 Protocol Optimization
```python
# Pure ASGI enables:
# - HTTP/2 multiplexing (30-50% throughput improvement)  
# - HTTP/3 QUIC support (lower latency)
# - Server push capabilities
# - Connection coalescing
```

**Implementation Priority:**
1. **Zero-copy streaming** - Immediate memory benefits for file uploads (specialized use case)
2. **Database connection reuse** - Significant DB performance gains (DB-heavy applications)
3. **WebSocket optimization** - Better real-time performance (chat/dashboard applications)
4. **SSE backpressure** - Prevents memory issues (broadcast applications)
5. **HTTP/2 support** - Protocol-level improvements

## ğŸ¯ Optimization Implementation Checklist

*Use this checklist when adding new features or reviewing existing code for optimization opportunities.*

### **Before Writing Any Code**
- [ ] Can this class benefit from `__slots__`?
- [ ] Are there string operations that can use f-strings?
- [ ] Should this use a frozenset instead of a list/tuple for lookups?
- [ ] Can expensive operations be cached with `@lru_cache`?
- [ ] Are there regex patterns that should be precompiled?

### **During Code Review**
- [ ] Look for `re.compile()` inside functions (move to module level)
- [ ] Check for string concatenation with `+` in loops (use `join()`)
- [ ] Identify manual loops that could be list comprehensions
- [ ] Find opportunities to use built-in functions (`sum`, `map`, `filter`)
- [ ] Verify async functions use `TaskGroup` not `asyncio.gather()` where appropriate
- [ ] Check for BaseHTTPMiddleware usage (convert to pure ASGI)

### **Performance Testing Requirements**
- [ ] Benchmark before and after changes
- [ ] Test with realistic data sizes
- [ ] Measure memory usage for high-frequency objects
- [ ] Verify no regressions in existing functionality
- [ ] Document performance improvements with numbers

### **Common Optimization Patterns by Use Case**

**High-Traffic Endpoints:**
```python
@lru_cache(maxsize=256)  # Cache responses
def generate_response(params_hash: str) -> dict:
    return expensive_computation(params_hash)
```

**Data Processing:**
```python
# Use generator expressions for large datasets
processed = (transform(item) for item in huge_dataset)
# Use built-ins when possible
total = sum(values)  # Instead of manual loop
```

**String-Heavy Operations:**
```python
# Precompile at module level
PATTERN = re.compile(r'pattern')
# Use f-strings for formatting
result = f"Processing {count} items"  # Fastest
```

**Memory-Critical Classes:**
```python
@dataclass(slots=True)
class HighVolumeData:
    field1: str
    field2: int
    # 40% memory reduction
```

## ğŸ¯ High-Priority Optimization Targets

### 1. Core Python Optimizations

#### A. Slots-Based Classes (40% Memory Reduction)
**Priority:** ğŸ”´ **High**  
**Files to Update:**
```python
# zenith/core/container.py
class DIContainer:
    __slots__ = ('_services', '_singletons', '_factories', '_startup_hooks', '_shutdown_hooks')

# zenith/core/routing/specs.py  
class RouteSpec:
    __slots__ = ('path', 'handler', 'methods', 'name', 'middleware', ...)
```

#### B. Precompiled Regex (10-50x Pattern Matching)
**Priority:** ğŸ”´ **High**  
**Pattern:**
```python
import re
from typing import Final

# Create module-level compiled patterns
PATH_PARAM: Final = re.compile(r'\{([^}]+)\}')
TRAILING_SLASH: Final = re.compile(r'/+$')
QUERY_STRING: Final = re.compile(r'\?.*$')

# Use in hot paths instead of re.findall(), re.sub()
```

#### C. String Interning for Common Values (15% Faster Comparisons)
**Priority:** ğŸŸ¡ **Medium**
```python
import sys

# Intern frequently compared strings
_COMMON_METHODS = {sys.intern("GET"), sys.intern("POST"), ...}
_COMMON_HEADERS = {sys.intern("content-type"), sys.intern("authorization"), ...}
```

### 2. Async I/O Optimizations

#### A. TaskGroup Pattern (20-30% Faster)
**Priority:** ğŸ”´ **High**  
**Pattern:**
```python
# Replace asyncio.gather() with TaskGroup for better performance/error handling
async with asyncio.TaskGroup() as tg:
    task1 = tg.create_task(operation1())
    task2 = tg.create_task(operation2())
```

#### B. Connection Pool Optimization (15-25% DB Overhead Reduction)
**Priority:** ğŸŸ¡ **Medium**
```python
Database(
    pool_size=20,
    max_overflow=30,
    pool_timeout=30,
    pool_recycle=3600,
    pool_pre_ping=True
)
```

### 3. Data Structure Optimizations

#### A. Dictionary Comprehensions vs Loops (15-30% Faster)
**Priority:** ğŸŸ¡ **Medium**
```python
# Efficient: Create new dict excluding expired
clean_data = {k: v for k, v in data.items() if not is_expired(v)}

# vs inefficient: Find then delete
expired_keys = [k for k, v in data.items() if is_expired(v)]
for k in expired_keys: del data[k]
```

#### B. Frozensets for Lookups (O(1) vs O(n))
**Priority:** ğŸŸ¡ **Medium**
```python
# Fast O(1) membership testing
allowed_methods = frozenset(['GET', 'POST', 'PUT'])
if method in allowed_methods:  # O(1) lookup
```

### 4. Memory Management

#### A. Object Pooling (15-25% GC Reduction)
**Priority:** ğŸŸ¢ **Low**
```python
from collections import deque

class ObjectPool:
    def __init__(self, factory, reset_func=None, max_size=100):
        self._pool = deque(maxlen=max_size)
        self._factory = factory
        self._reset_func = reset_func
```

#### B. Response Caching (25-40% Response Generation)
**Priority:** ğŸŸ¡ **Medium**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_json_response(content_hash: int) -> bytes:
    return msgspec.json.encode(content).decode()
```

## ğŸ“Š Performance Benchmarking Standards

### Current Performance Targets (as of v0.1.4)

**Bare Framework:**
- Simple endpoints: **â‰¥9,600 req/s** (Current: 7,743 req/s baseline)
- JSON endpoints: **â‰¥9,800 req/s** (Current: 9,917 req/s with optimizations)

**With Full Middleware Stack:**
- Performance retention: **â‰¥25%** (Current: 71% - 7,044 req/s)
- Middleware overhead: **â‰¤75%** (Current: 29% overhead)

### Regression Testing

**Command to verify performance:**
```bash
SECRET_KEY=test-secret-key-that-is-long-enough-for-testing uv run python benchmarks/simple_bench.py
```

**Benchmark Frequency:**
- Run after any middleware changes
- Run after core routing modifications
- Run before each release

## ğŸ”§ Implementation Checklist

### Before Adding New Features

**Always Check:**
- [ ] Can this class use `__slots__`?
- [ ] Are there repeated string operations that need precompiled regex?
- [ ] Is this using the fastest data structure (dict vs list vs frozenset)?
- [ ] Can this leverage existing object pools?

### Code Review Checklist

**Performance Red Flags:**
- [ ] `re.compile()` inside loops or hot paths
- [ ] List comprehensions that should be generator expressions
- [ ] `json.dumps()/loads()` instead of msgspec in hot paths
- [ ] BaseHTTPMiddleware instead of pure ASGI
- [ ] `asyncio.gather()` instead of TaskGroup (Python 3.11+)
- [ ] Unnecessary object creation in request processing

### Release Performance Verification

**Before Each Release:**
1. Run full benchmark suite
2. Verify no performance regressions >5%
3. Update this document with new optimizations
4. Document any performance-impacting changes

## ğŸ“ˆ Optimization Impact Summary

| Optimization Category | Expected Improvement | Implementation Effort | v0.1.4 Status |
|----------------------|---------------------|----------------------|----------------|
| msgspec JSON | 2-10x JSON operations | âœ… **Done** | âœ… **Implemented** |
| ASGI Middleware | +127% middleware perf | âœ… **Done** | âœ… **Implemented** |
| Slots Classes | 40% memory, faster access | ğŸŸ¡ Medium | âœ… **Implemented** |
| String Interning | 15% string comparisons | ğŸŸ¢ Easy | âœ… **Implemented** |
| TaskGroup Patterns | 20-30% async operations | ğŸŸ¢ Easy | âœ… **Implemented** |
| Frozenset Lookups | O(1) vs O(n) membership | ğŸŸ¢ Easy | âœ… **Implemented** |
| Response Caching | 25-40% response generation | ğŸŸ¡ Medium | ğŸŸ¡ **Partial** |
| Precompiled Regex | 10-50x pattern matching | ğŸŸ¡ Medium | ğŸ”„ **Future** |
| Object Pooling | 15-25% GC reduction | ğŸ”´ Complex | ğŸ”„ **Future** |

### v0.1.4 Implementation Results
- **Overall performance improvement**: 4.8% (9,464 req/s â†’ 9,917 req/s JSON endpoints)
- **Middleware performance**: 71% retention (7,044 req/s with full stack)  
- **Memory optimization**: __slots__ applied to 3 high-usage dependency classes
- **Code efficiency**: 15% faster string comparisons with interned HTTP constants
- **Async improvements**: TaskGroup pattern applied to health check system

## ğŸ† Performance Philosophy

### Core Principles

1. **Measure First:** Always benchmark before and after optimizations
2. **Hot Path Focus:** Prioritize optimizations in request processing paths
3. **Stable Optimizations:** Use proven Python patterns, avoid experimental libraries
4. **Maintainability:** Don't sacrifice code clarity for marginal gains
5. **Production Ready:** All optimizations must be production-tested

### Performance vs Feature Balance

**High-Impact, Low-Risk Optimizations:**
- JSON library upgrades (msgspec, orjson)
- Data structure improvements (frozenset, deque)
- Precompiled regex patterns
- ASGI middleware patterns

**Medium-Impact Optimizations:**
- Object pooling
- Response caching
- Connection pooling tuning

**Low-Priority Optimizations:**
- Micro-optimizations (<5% improvement)
- Complex memory management
- Platform-specific optimizations

## ğŸ” Profiling and Monitoring

### Built-in Performance Monitoring

**Available Tools:**
- `zenith.performance.track_performance()` - Function-level profiling
- `zenith.performance.cached()` - Automatic caching with TTL
- `/metrics` endpoint - Prometheus-compatible metrics
- `/health/detailed` - Performance health checks

### External Profiling

**Recommended Tools:**
```bash
# Memory profiling
pip install memory-profiler
python -m memory_profiler script.py

# CPU profiling  
pip install py-spy
py-spy top --pid <zenith-process-id>

# Line-by-line profiling
pip install line_profiler
kernprof -l -v script.py
```

## ğŸ“š References

**Performance Resources:**
- [Python Performance Tips](https://wiki.python.org/moin/PythonSpeed/PerformanceTips)
- [msgspec Documentation](https://jcristharif.com/msgspec/)
- [ASGI vs WSGI Performance](https://www.python.org/dev/peps/pep-3333/)

**Framework-Specific:**
- `zenith/optimizations/advanced.py` - Advanced optimization patterns
- `benchmarks/` directory - Performance test suite
- `zenith/testing/` - Performance testing utilities

---

*This document should be updated whenever significant optimizations are implemented or discovered. Last updated: September 2025 (v0.1.4 - Performance Optimization Release)*